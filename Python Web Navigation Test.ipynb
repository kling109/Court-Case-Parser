{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Python Web Navigation </center> </h1>\n",
    "<center> <h3> HireRite Grand Challege </center> </h3>\n",
    "<center> <h5> Matt Raymond </center> </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web driver for comtroling chrome\n",
    "from selenium import webdriver\n",
    "\n",
    "# Regex for parsing website navigation\n",
    "import re as regx\n",
    "\n",
    "# Thread sleeping\n",
    "from time import sleep\n",
    "\n",
    "# Import for dates\n",
    "from datetime import datetime\n",
    "\n",
    "# Import keys so we can press enter\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Wait function so the webpage an finish loading\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from parse import ImageParsing\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import math\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of the path of the chrome driver\n",
    "# ToDO: Fix the chrome driver path so it doesn't need to be set automatically inside python\n",
    "__BROWSER__ = webdriver.Chrome(executable_path='/home/trevor/Documents/HireRightChallenge/test_reassembly/chromedriver_linux64/chromedriver')\n",
    "\n",
    "# Set the implicit wait time so that webpages don't flip out when they can't fontrol things right away\n",
    "__BROWSER__.implicitly_wait(5)\n",
    "\n",
    "__PARSING__ = ImageParsing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Testing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The websites that we're supposed to use\n",
    "__TESTINGWEBSITES__ = [\"https://www.sccourts.org/casesearch/\", # agreement says no crawlers\n",
    "                       \"https://wcca.wicourts.gov/case.html\", # added captcha\n",
    "                       \"https://www.oscn.net/dockets/search.aspx\",\n",
    "                       \"https://www.civitekflorida.com/ocrs/county/\", # recapcha\n",
    "                       \"https://apps.supremecourt.az.gov/publicaccess/caselookup.aspx\", # captcha\n",
    "                       \"https://www.superiorcourt.maricopa.gov/docket/CriminalCourtCases/caseSearch.asp\",\n",
    "                       \"http://justicecourts.maricopa.gov/FindACase/casehistory.aspx\",\n",
    "                       \"https://casesearch.epcounty.com/PublicAccess/default.aspx\"]\n",
    "\n",
    "# the websites that are actually LEGAL to use\n",
    "__LEGALWEBSITES__ = __TESTINGWEBSITES__[2:3] + __TESTINGWEBSITES__[-3:]\n",
    "\n",
    "# Testing variables for name and date of birth\n",
    "__TESTING_FNAME__ = \"John\"\n",
    "__TESTING_LNAME__ = \"Smith\"\n",
    "__TESTING_MNAME__ = \"Herbert\"\n",
    "__TESTING_DOB__ = \"09-11-2001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex for different searches\n",
    "__FIRSTNAME__ = \"=\\\"[^\\\"]*[Ff][A-Za-z]*[Nn]ame[A-Za-z]*\\\"\"\n",
    "__LASTNAME__ = \"=\\\"[^\\\"]*[Ll][A-Za-z]*[Nn]ame[A-Za-z]*\\\"\"\n",
    "__MIDDLENAME__ = \"=\\\"[^\\\"]*[Mm][A-Za-z]*[Nn]ame[A-Za-z]*\\\"\"\n",
    "__BIRTHDATE__ = \"=\\\"[^\\\"]*?(search)?[^\\\"]*d(ate)?of?b(irth)?[^\\\"^(search)]*?\\\"\"\n",
    "__DATEFORMAT__ = \"[MD\\d]{2}[./-][MD\\d]{2}[./-][Y\\d]{4}\"\n",
    "__REACTFORMAT__ = \"<div id=\\\"[^\\\"]*react[^\\\"]*\\\"><\\/div>\"\n",
    "__OTHER_LOADING_FORMAT__ = \"svgclippaths\\\">\\<head>\\<meta\"\n",
    "__SEARCHBUTTON__ = \"=\\\"[^\\\"]*?((?<!menu)(?<!re)(?<!box)search)[^\\\"]*?\\\"\"\n",
    "__CASELINK__ = \"(?<=href=\\\")([^\\\"]*((number)|(case((no)|(id))))[^\\\"]*)(?=\\\")\"\n",
    "__NEXT_PAGE_ID__ = \"(?<=id=\\\")[^\\\"]*next(?=\\\")\"\n",
    "__NEXT_PAGE_HREF__ = \"(?<=href=\\\")[^\\\"]*(?=\\\">next)\"\n",
    "__URL_PAGE_NUMBER__ = \"(?<=page=)\\d*(?=&)\"\n",
    "__COUNTY__ = \"(([A-Z][a-zA-Z\\-]*\\ )|([a-zA-Z\\-]*(&nbsp;)+)){1,3}[Cc][Oo][Uu][Nn][Tt][Yy]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Info from Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scrapes the url from a webpage\n",
    "Input: takes a string strURL as a url to a webpage\n",
    "Output: returns the html from the webpage as a string\n",
    "'''\n",
    "def getSource(strURL):\n",
    "    __BROWSER__.get(strURL)\n",
    "    print(\"Scraping website source code...\")\n",
    "    return pauseforReact(__BROWSER__.page_source)\n",
    "\n",
    "'''\n",
    "Waits for react elements to load so that we have all of the data loaded before scraping\n",
    "Input: takes a string html from the webpage to check for react elements\n",
    "Output: returns the final html source as a string\n",
    "'''\n",
    "def pauseforReact(html):\n",
    "    while regx.search(__REACTFORMAT__, html, regx.IGNORECASE) is not None and regx.search(__OTHER_LOADING_FORMAT__, html, regx.IGNORECASE) is None:\n",
    "        sleep(1)\n",
    "        html = __BROWSER__.page_source\n",
    "    return __BROWSER__.page_source\n",
    "\n",
    "'''\n",
    "Searches for the name of an element given a regex description\n",
    "Input: takes a string ws as the source html and a string re as the regex search criteria\n",
    "Output: returns the name of the element found as a string\n",
    "'''\n",
    "def regSearchName(ws, re):\n",
    "    # Searches for a tag with a given name\n",
    "    _result = regx.search(\"name\" + re, ws, regx.IGNORECASE)\n",
    "    if(_result is None):\n",
    "        return \"\";\n",
    "    else:\n",
    "        # Return the result minus the \"name=\" part\n",
    "        return _result[0][6:-1]\n",
    "\n",
    "'''\n",
    "Searches for the ID of an element given a regex description\n",
    "Input: takes a string ws as the source html and a string re as the regex search criteria\n",
    "Output: returns the name of the element found as a string\n",
    "'''\n",
    "def regSearchID(ws, re):\n",
    "    _result = regx.search(\"id\" + re, ws, regx.IGNORECASE)\n",
    "    if(_result is None):\n",
    "        return \"\";\n",
    "    else:\n",
    "        return _result[0][4:-1]\n",
    "\n",
    "'''\n",
    "Finds the id and name of the object that meets a certain search criteria\n",
    "Input: takes a string ws as the source html and a string re as the regex search criteria\n",
    "Output: returns the id and name of the located element as a tuple\n",
    "  - This allows for our logic to include cases where the name field is filled and the id isn't or vice versa\n",
    "'''\n",
    "def getIDandName(ws, re):\n",
    "    return (regSearchID(ws, re), regSearchName(ws, re))\n",
    "\n",
    "'''\n",
    "Finds the county name on the page\n",
    "Input: takes a string html as the page html\n",
    "Output: returns a string for the county\n",
    "'''\n",
    "def findCountyName(html):\n",
    "    states = [\"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"colorado\", \"connecticut\", \"delaware\",\n",
    "        \"florida\", \"georgia\", \"hawaii\", \"idaho\", \"illinois\", \"indiana\", \"iowa\", \"kansas\", \"kentucky\",\n",
    "        \"louisiana\", \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\", \"mississippi\", \"missouri\",\n",
    "        \"montana\", \"nebraska\", \"nevada\", \"new hampshire\", \"new jersey\", \"new mexico\", \"new york\",\n",
    "        \"north carolina\", \"north dakota\", \"ohio\", \"oklahoma\", \"oregon\", \"pennsylvania\", \"rhode island\",\n",
    "        \"south carolina\", \"south dakota\", \"tennessee\", \"texas\", \"utah\", \"vermont\", \"virginia\", \"washington\",\n",
    "        \"west Virginia\", \"wisconsin\", \"wyoming\"]\n",
    "    \n",
    "    _result = regx.search(__COUNTY__, html)\n",
    "    if(_result):\n",
    "        _result = _result[0].replace(\"&nbsp;\", \" \").lower()\n",
    "        \n",
    "        # Removes the state name from the title if it's included\n",
    "        for i in states:\n",
    "            _result = _result.replace(i+\" \", \"\")\n",
    "        return _result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fills out the person's name on the website\n",
    "Input: takes a string html as the website's source html, string fn as the first name,\n",
    "       a string ln as the last name, and a string mn as the middle name\n",
    "Output: None\n",
    "'''\n",
    "def fillOutName(html, fn, ln, mn):\n",
    "    # Finds the ID and Name for first, middle, and last names\n",
    "    fnID, fnName = getIDandName(html, __FIRSTNAME__)\n",
    "    mnID, mnName = getIDandName(html, __MIDDLENAME__)\n",
    "    lnID, lnName = getIDandName(html, __LASTNAME__)\n",
    "\n",
    "    sendInput(fnName, fn)\n",
    "    sendInput(mnName, mn)\n",
    "    sendInput(lnName, ln)\n",
    "\n",
    "'''\n",
    "Sends input to an element\n",
    "Input: takes a string e as a web element name and a string s as the text to be sent\n",
    "Output: None\n",
    "Exception: prints an error message saying what text it tried to enter into what element\n",
    "'''\n",
    "def sendInput(e, s):\n",
    "    try:\n",
    "        # Breaks out if there's no element sent\n",
    "        if str(e) is \"\":\n",
    "            return\n",
    "        \n",
    "        # Find the element based on the name and send input\n",
    "        item = __BROWSER__.find_element_by_name(e)\n",
    "        item.click()\n",
    "        item.clear()\n",
    "        item.send_keys(s)\n",
    "        \n",
    "    except:\n",
    "        print(\"Input failed. Attempted to enter \\\"{0}\\\" to \\\"{1}\\\" and failed.\".format(s, e))\n",
    "    \n",
    "# Fill out the date of birth\n",
    "# Returns a bool denoting whether it was successful or not\n",
    "\n",
    "'''\n",
    "Fills out the date of birth on the page\n",
    "Input: takes a string html as the website source html and a string dob\n",
    "Output: returns a bool saying whether it was successful or not\n",
    "Exception: prints a message saying that it failed and what it tried to enter as a dob\n",
    "'''\n",
    "def fillOutDOB(html, dob):\n",
    "    try:\n",
    "        # If there is a dob given\n",
    "        if dob is not None:\n",
    "            # get the id and name of the date-picker element\n",
    "            dID, dName = getIDandName(html, __BIRTHDATE__)\n",
    "        \n",
    "            # Try to find the element by name because this is more common\n",
    "            if dName is not \"\":\n",
    "                item = __BROWSER__.find_element_by_name(dName)\n",
    "            # If it can't find it, try by id\n",
    "            else:\n",
    "                item = __BROWSER__.find_element_by_id(dID)\n",
    "            \n",
    "            # If the item is locked then there's no poin in trying to edit it\n",
    "            if not item.is_enabled():\n",
    "                return False\n",
    "            \n",
    "            # Select the element and remove all of the information already in it\n",
    "            item.click()\n",
    "            item.clear()\n",
    "            \n",
    "            # Find the delimiter that the webpage wants us to use (in case it's picky)\n",
    "            _delin = findDateFormat(html)\n",
    "            \n",
    "            # Create a date based on the american standard (since these are american websites),\n",
    "            # using the specified delimiters\n",
    "            _date = \"{1}{0}{2}{0}{3}\".format(_delin, str(dob.month).zfill(2), str(dob.day).zfill(2), dob.year)\n",
    "            \n",
    "            # Fill out the date\n",
    "            item.send_keys(_date)\n",
    "        return True\n",
    "    \n",
    "    # If something goes wrong and the date input can't be found or can't be edited\n",
    "    except:\n",
    "        print(\"Input failed. Attempted to enter \\\"{0}\\\" as a date and failed.\".format(dob))\n",
    "        return False\n",
    "        \n",
    "# Find the delimiters used in the example (if one exists) and returns them\n",
    "\n",
    "'''\n",
    "Finds the date format being used on the page. This is to ensure that the crawler doesn't\n",
    "get stuck because of overly-picky websites\n",
    "Input: takes a string html as the website's source html\n",
    "Output: returns a string as either the found delimeter, or a dash as the default\n",
    "'''\n",
    "def findDateFormat(html):\n",
    "    # Find the example date on the page\n",
    "    result = regx.search(__DATEFORMAT__, html, regx.IGNORECASE)\n",
    "    \n",
    "    # Return either the result or use a random one that I happened to pick\n",
    "    if result is not None:\n",
    "        return result[0][2]\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "# Initialize the search\n",
    "\n",
    "'''\n",
    "Submits the search criteria\n",
    "Input: takes a string html as the website source html\n",
    "Output: None\n",
    "'''\n",
    "def search(html):\n",
    "    # Saves the url for a comparison later\n",
    "    lastURL = __BROWSER__.current_url\n",
    "    \n",
    "    # Gets the id and name of the first name field\n",
    "    fnID, fnName = getIDandName(html, __FIRSTNAME__)\n",
    "    \n",
    "    # Tries pressing enter to submit\n",
    "    # Some websites have search buttons that are really difficult to find,\n",
    "    # so this makes it a lot easier\n",
    "    __BROWSER__.find_element_by_name(fnName).send_keys(Keys.ENTER)\n",
    "    \n",
    "    # If we didn't redirect, then we need to actually click the search button\n",
    "    if lastURL == __BROWSER__.current_url :\n",
    "        # Gets the id and name of the search button\n",
    "        sID, sName = getIDandName(html, __SEARCHBUTTON__)\n",
    "\n",
    "        # If there is a search button within a reasonable distance\n",
    "        if (sID is not \"\" or sName is not \"\") and (sID in html or sName in html):\n",
    "            try:\n",
    "                # Find and click the button\n",
    "                item = __BROWSER__.find_element_by_name(sName)\n",
    "                item.click()\n",
    "            except:\n",
    "                return\n",
    "    \n",
    "'''\n",
    "Fills out all of the information given\n",
    "Input: takes in a stirng html as the website source html, a string fn as the first name,\n",
    "       a string ln as the last name, a string mn as the middle name (defaulted to en empty string),\n",
    "       and a date dob as the date of birth (defaulted to None)\n",
    "Output: None\n",
    "'''\n",
    "def fillOutInformation(html, fn, ln, mn = \"\", dob = None):\n",
    "    # NOTE: The fields must be filled out in this order or sometimes\n",
    "    # the open date selector messes with the submission\n",
    "    \n",
    "    # Variable that checks whether the dob was filled out correctly\n",
    "    dobFilled = fillOutDOB(html, dob)\n",
    "    \n",
    "    fillOutName(html, fn, ln, mn)\n",
    "    \n",
    "    # Sometimes the dob section is locked until you enter a name,\n",
    "    # so this checks to see if the date of birth was filled out or not\n",
    "    if not dobFilled:\n",
    "        fillOutDOB(html, dob)\n",
    "\n",
    "'''\n",
    "Formats a html url to correctly format special characters\n",
    "Input: takes a string s as a url\n",
    "Output: returns a string as a url\n",
    "'''\n",
    "def formatURL(s):\n",
    "    return s.replace(\"&amp;\", \"&\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cycles through all of the cases on a page, opens the links, and returns back to the page at the end\n",
    "Input: takes a string currentURL as the url of the page currently active, and an int numberOfTimes\n",
    "       as a testing variable that says how many links should be run through. It is defaulted to -1,\n",
    "       which is infinite\n",
    "Output: None\n",
    "'''\n",
    "def cycleThroughCases(currentURL, landingCounty, numberOfTimes = -1):\n",
    "    # Waits intil the results page has actually loaded\n",
    "    while __BROWSER__.current_url == currentURL:\n",
    "        sleep(1)\n",
    "    \n",
    "    # Waits for all of the react elements to load\n",
    "    html = pauseforReact(__BROWSER__.page_source)\n",
    "    \n",
    "    # Find all of the links to cases\n",
    "    results = list(regx.findall(__CASELINK__, html, regx.IGNORECASE))\n",
    "    \n",
    "    # save the current url so we can redirect back\n",
    "    currentURL = __BROWSER__.current_url\n",
    "    \n",
    "    # Not sure why, but sometimes a tuple is returned and we have to take that into account\n",
    "    isTuple = type(results[0]) is tuple\n",
    "    \n",
    "    # VIterator variable for comparing against numberOfTimes\n",
    "    i = 0\n",
    "    \n",
    "    # Selects the first element if it's in a tuple\n",
    "    if isTuple:\n",
    "        results = [r[0] for r in results]\n",
    "            \n",
    "    # Removes duplicate links\n",
    "    results = list(dict.fromkeys(results))\n",
    "        \n",
    "    searchCounty = findCountyName(html)\n",
    "        \n",
    "    # Cycles through all results\n",
    "    for r in results:\n",
    "        i += 1\n",
    "                    \n",
    "        # Skip over links to css files\n",
    "        if \".css\" not in r:\n",
    "            print(\"Scraping {0}\".format(r))\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                # Find the element based on the link\n",
    "                __BROWSER__.execute_script(\"window.open(\\\"\" + formatURL(r) + \"\\\",\\\"_blank\\\")\")\n",
    "                __BROWSER__.switch_to.window(__BROWSER__.window_handles[1])\n",
    "                \n",
    "                resultCounty = findCountyName(html)\n",
    "                \n",
    "                lstCounties = [landingCounty, searchCounty, resultCounty]\n",
    "                \n",
    "                # ThIs Is WhErE tHe WeBsCrApInG sHoUlD gO\n",
    "                # Pass in lstCounty\n",
    "                imgString = save_fullpage_screenshot(__BROWSER__, \"./Screenshots/tmp.png\")\n",
    "                print(ParseImage(imgString, lstCounties))\n",
    "                \n",
    "\n",
    "                sleep(2) # Debug: So we can see the page\n",
    "\n",
    "                # Redirect back to the page\n",
    "                __BROWSER__.close()\n",
    "                __BROWSER__.switch_to.window(__BROWSER__.window_handles[0])\n",
    "                sleep(2) # Debug: So we can see the page\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "            # Breaks for testing purposes\n",
    "            if(i == numberOfTimes):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Figures out if there's a next page and switches to it if there is\n",
    "Input: takes a bool sc that tells whether we should click (true) or whether\n",
    "       we should edit the url (false)\n",
    "Output: returns a bool that tells whether the page can redirect again (true)\n",
    "        or whether this is the last page (false)\n",
    "'''\n",
    "def findNextPage(sc):\n",
    "    # Saves the html of the current page\n",
    "    currentHTML = __BROWSER__.page_source\n",
    "    \n",
    "    # If we're going the clicking route\n",
    "    if sc:\n",
    "        # Find all of the elements that can be used to get to the next page\n",
    "        _result = regx.findall(__NEXT_PAGE_ID__, currentHTML, regx.IGNORECASE)\n",
    "        \n",
    "        # If that returns anything\n",
    "        if len(_result) is not 0:\n",
    "            # Find the first element\n",
    "            nextButton = __BROWSER__.find_element_by_id(_result[0])\n",
    "            \n",
    "            # Check to see if the element is disabled\n",
    "            returnValue = (\"disabled\" not in nextButton.get_attribute(\"class\"))\n",
    "            \n",
    "            # Click 'cause why not\n",
    "            nextButton.click()\n",
    "            \n",
    "            # Return whether it was disabled\n",
    "            return returnValue\n",
    "        \n",
    "        # If we're going to click a link \n",
    "        else:\n",
    "            # Find the links\n",
    "            _result = regx.findall(__NEXT_PAGE_HREF__, currentHTML, regx.IGNORECASE)\n",
    "\n",
    "            # If there are any\n",
    "            if len(_result) is not 0:\n",
    "                # Find the element and click it\n",
    "                __BROWSER__.find_element_by_xpath('//a[@href=\"'+formatURL(_result[0])+'\"]').click()\n",
    "                return True\n",
    "            # Otherwise return false\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cycles through all pages\n",
    "\n",
    "'''\n",
    "Cycles through all pages and scrapes all of them\n",
    "Input: takes an int noOfPages which tells how many pages to cycle through (for debugging).\n",
    "       The default is -1, which means infinite\n",
    "Output: None\n",
    "'''\n",
    "def cycleThroughAllPages(landingCounty, noOfPages = -1):\n",
    "    \n",
    "    html = __BROWSER__.page_source\n",
    "    \n",
    "    # Find whether there is a link for the next page or not \n",
    "    _result = regx.findall(__NEXT_PAGE_HREF__, html, regx.IGNORECASE)\n",
    "        \n",
    "    # If there's a link, then you should click it\n",
    "    shouldClick = (len(_result) > 0)\n",
    "    \n",
    "    url = \"\"\n",
    "    \n",
    "    # While we're still supposed to continue\n",
    "    while noOfPages != 0:\n",
    "        \n",
    "        cycleThroughCases(url, landingCounty, 5) ##randomthingy\n",
    "        url = __BROWSER__.current_url\n",
    "        # If we're not supposed to continue, then break\n",
    "        if not findNextPage(shouldClick):\n",
    "            break;\n",
    "\n",
    "#         sleep(2) # Debugging \n",
    "        noOfPages -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigateElPaso():\n",
    "    if(\"epcounty\" in __BROWSER__.current_url):\n",
    "        __BROWSER__.find_elements_by_class_name(\"ssSearchHyperLink\")[0].click()\n",
    "        __BROWSER__.find_element_by_xpath(\"//select[@name='SearchBy']/option[text()='Defendant']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanWebsite(ws, fn, ln, mn = \"\", dob = \"\"):\n",
    "    __BROWSER__.switch_to.window(__BROWSER__.window_handles[0])\n",
    "    __BROWSER__.get(ws)\n",
    "    navigateElPaso()\n",
    "    html = __BROWSER__.page_source\n",
    "    \n",
    "    try:\n",
    "        convertedDate = datetime.strptime(dob, '%m-%d-%Y')\n",
    "    except:\n",
    "        convertedDate = None\n",
    "     \n",
    "    fillOutInformation(html, fn, ln, mn, convertedDate)\n",
    "    search(html)\n",
    "    try:\n",
    "        landingCounty = findCountyName(html)\n",
    "        cycleThroughAllPages(landingCounty, 2)\n",
    "    except:\n",
    "        print(\"There were no cases for this person\")\n",
    "    \n",
    "def ParseImage(strFilepath, lstCounties):\n",
    "    return __PARSING__.parseData(strFilepath, lstCounties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://arthurpemberton.com/2017/12/full-page-screenshots-with-python-and-selenium\n",
    "\n",
    "def save_fullpage_screenshot(driver, output_path, tmp_prefix='selenium_screenshot', tmp_suffix='.png'):\n",
    "    \"\"\"\n",
    "    Creates a full page screenshot using a selenium driver by scrolling and taking multiple screenshots,\n",
    "    and stitching them into a single image.\n",
    "    \"\"\"\n",
    " \n",
    "#     # get the page\n",
    "#     driver.get(url)\n",
    " \n",
    "    # get dimensions\n",
    "    window_height = driver.execute_script('return window.innerHeight')\n",
    "    scroll_height = driver.execute_script('return document.body.parentNode.scrollHeight')\n",
    "    num = int( math.ceil( float(scroll_height) / float(window_height) ) )\n",
    " \n",
    "    # get temp files\n",
    "    tempfiles = []\n",
    "    for i in range( num ):\n",
    "        fd,path = tempfile.mkstemp(prefix='{0}-{1:02}-'.format(tmp_prefix, i+1), suffix=tmp_suffix)\n",
    "        os.close(fd)\n",
    "        tempfiles.append(path)\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        # take screenshots\n",
    "        for i,path in enumerate(tempfiles):\n",
    "            if i > 0:\n",
    "                driver.execute_script( 'window.scrollBy(%d,%d)' % (0, window_height) )\n",
    "             \n",
    "            driver.save_screenshot(path)\n",
    "            pass\n",
    "         \n",
    "        # stitch images together\n",
    "        stiched = None\n",
    "        for i,path in enumerate(tempfiles):\n",
    "            img = Image.open(path)\n",
    "             \n",
    "            w, h = img.size\n",
    "            y = i * window_height\n",
    "             \n",
    "            if i == ( len(tempfiles) - 1 ):\n",
    "                img = img.crop((0, h-(scroll_height % h), w, h))\n",
    "                w, h = img.size\n",
    "                pass\n",
    "             \n",
    "            if stiched is None:\n",
    "                stiched = Image.new('RGB', (w, scroll_height))\n",
    "             \n",
    "            stiched.paste(img, (\n",
    "                0, # x0\n",
    "                y, # y0\n",
    "                w, # x1\n",
    "                y + h # y1\n",
    "            ))\n",
    "            pass\n",
    "        stiched.save(output_path)\n",
    "    finally:\n",
    "        # cleanup\n",
    "        for path in tempfiles:\n",
    "            if os.path.isfile(path):\n",
    "                os.remove(path)\n",
    "        pass\n",
    " \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0693a98418a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscanWebsite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__LEGALWEBSITES__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__TESTING_FNAME__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__TESTING_LNAME__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "scanWebsite(__LEGALWEBSITES__[0], __TESTING_FNAME__, __TESTING_LNAME__, \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
